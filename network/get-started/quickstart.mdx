---
title: "Quickstart"
description: "Step-by-step guide to make an API call to interact with an LLM Model"
icon: "rocket"
iconType: "light"
---

## Step 1: Install via pip

Run the following command in your terminal to install the SDK:

```bash
pip install mira-network
```

This will download and install the latest version of the Mira SDK along with its dependencies.

## Step 2: Verify Installation

After installation, you can verify that the SDK is correctly installed by running:

```bash
pip show mira-network
```

You should see the SDK version and related metadata.

## Step 3: Import the SDK

Test your setup by creating a simple Python script and importing the Mira SDK

### Synchronous Usage

```python Python
from mira_network.sync_client import MiraSyncClient
from mira_network.models import AiRequest, Message

# Using context manager (recommended)
with MiraSyncClient(api_token="your_api_token", base_url="https://apis.mira.network") as client:
    # Example 1: Non-streaming response
    request = AiRequest(
        messages=[
            Message(role="system", content="You are a helpful assistant."),
            Message(role="user", content="Hello!")
        ],
        model="gpt-4o",                                                        # You can choose from the available models
        stream=False
    )
    response = client.generate(request)
    print(response)
```

### Asynchronous Usage

```python Python
import asyncio
from mira_network.client import MiraClient
from mira_network.models import AiRequest, Message, ModelProvider

async def main():
    # Using async context manager (recommended)
    async with MiraClient(api_token="your-api-token") as client:
        # Example 1: Non-streaming response
        request = AiRequest(
            messages=[
                Message(role="system", content="You are a helpful assistant."),
                Message(role="user", content="Hello!")
            ],
            model="gpt-4o",
            stream=False
        )
        response = await client.generate(request)
        print(response)
        
        # Example 2: Streaming response
        stream_request = AiRequest(
            messages=[
                Message(role="system", content="You are a helpful assistant."),
                Message(role="user", content="Tell me a story!")
            ],
            model="gpt-4o",
            stream=True
        )
        async for chunk in await client.generate(stream_request):
            print(chunk)

if __name__ == "__main__":
    asyncio.run(main())
```

You're now ready to use the Mira SDK to build powerful AI-driven applications!
